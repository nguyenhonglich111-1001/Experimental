feat(evaluation): Create standalone RAG evaluation harness

This commit introduces a new, standalone RAG (Retrieval-Augmented Generation) evaluation harness. This framework is designed to be generic and reusable for benchmarking any RAG system.

The harness is located in the `rag-eval-harness/` directory and includes:
- A configurable main runner (`main.py`, `config.yaml`).
- A dataset downloader and processor (`datasets/download.py`) configured for the NarrativeQA benchmark.
- A metrics module (`evaluators/metrics.py`) for calculating standard IR and generation scores (Precision, Recall, MRR, ROUGE, BERTScore).
- An extensible retriever interface (`retrievers/base.py`) with a concrete implementation for the existing `langchain-file-processor` (`retrievers/langchain_pdf_retriever.py`).

This provides a robust system for measuring both the effectiveness and efficiency of our RAG pipelines, enabling data-driven development and regression testing.